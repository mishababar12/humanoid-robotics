"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9880],{5358(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"gazebo_unity/labs/lab3.1","title":"Lab 3.1: Simulating a Mobile Robot and Streaming Sensor Data in Gazebo","description":"This lab focuses on setting up a Gazebo simulation for a mobile robot and configuring it to stream various sensor data to ROS 2 topics. This is a crucial step for developing and testing robot perception and navigation algorithms.","source":"@site/docs/gazebo_unity/labs/lab3.1.md","sourceDirName":"gazebo_unity/labs","slug":"/gazebo_unity/labs/lab3.1","permalink":"/humanoid-robotics/ur/docs/gazebo_unity/labs/lab3.1","draft":false,"unlisted":false,"editUrl":"https://github.com/mishababar12/humanoid-robotics/tree/main/my-website/docs/gazebo_unity/labs/lab3.1.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Robot Simulation with Gazebo & Unity","permalink":"/humanoid-robotics/ur/docs/gazebo_unity/"},"next":{"title":"Unity Visualization: Bringing Robots to Life with Advanced Graphics","permalink":"/humanoid-robotics/ur/docs/gazebo_unity/unity_visualization"}}');var o=s(4848),a=s(8453);const r={},t="Lab 3.1: Simulating a Mobile Robot and Streaming Sensor Data in Gazebo",l={},d=[{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Instructions",id:"instructions",level:2},{value:"Step 1: Launch the TurtleBot3 Gazebo Simulation",id:"step-1-launch-the-turtlebot3-gazebo-simulation",level:3},{value:"Step 2: Inspect ROS 2 Topics",id:"step-2-inspect-ros-2-topics",level:3},{value:"Step 3: Analyze Sensor Data",id:"step-3-analyze-sensor-data",level:3},{value:"Step 4: Visualizing the Robot and Sensor Data in RViz2",id:"step-4-visualizing-the-robot-and-sensor-data-in-rviz2",level:3},{value:"Step 5: Recording Data (Optional)",id:"step-5-recording-data-optional",level:3},{value:"Experimentation and Analysis",id:"experimentation-and-analysis",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"lab-31-simulating-a-mobile-robot-and-streaming-sensor-data-in-gazebo",children:"Lab 3.1: Simulating a Mobile Robot and Streaming Sensor Data in Gazebo"})}),"\n",(0,o.jsx)(n.p,{children:"This lab focuses on setting up a Gazebo simulation for a mobile robot and configuring it to stream various sensor data to ROS 2 topics. This is a crucial step for developing and testing robot perception and navigation algorithms."}),"\n",(0,o.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Launch a pre-configured Gazebo simulation environment with a mobile robot."}),"\n",(0,o.jsx)(n.li,{children:"Verify the existence of key ROS 2 topics for control and sensor data."}),"\n",(0,o.jsx)(n.li,{children:"Record and analyze simulated sensor data (e.g., laser scan, odometry, camera feed)."}),"\n",(0,o.jsx)(n.li,{children:"Understand the basic structure of a robot model in Gazebo and how sensors are integrated."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A functional ROS 2 Humble/Iron installation."}),"\n",(0,o.jsxs)(n.li,{children:["A ROS 2 workspace set up (e.g., ",(0,o.jsx)(n.code,{children:"~/ros2_ws"}),")."]}),"\n",(0,o.jsx)(n.li,{children:"Basic understanding of Gazebo environments (from previous section)."}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"A simulated robot environment with sensors:"})," We will use the TurtleBot3 simulation in Gazebo as it comes pre-configured with a differential drive robot, a 2D laser scanner, a camera, and publishes odometry.","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"sudo apt install ros-humble-turtlebot3-gazebo ros-humble-turtlebot3-description\n"})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"instructions",children:"Instructions"}),"\n",(0,o.jsx)(n.h3,{id:"step-1-launch-the-turtlebot3-gazebo-simulation",children:"Step 1: Launch the TurtleBot3 Gazebo Simulation"}),"\n",(0,o.jsxs)(n.p,{children:["Open a terminal and launch the ",(0,o.jsx)(n.code,{children:"turtlebot3_world"})," simulation:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py\n"})}),"\n",(0,o.jsx)(n.p,{children:"This command will open the Gazebo simulator with the TurtleBot3 in a simple world. You should see the robot and potentially some obstacles."}),"\n",(0,o.jsx)(n.h3,{id:"step-2-inspect-ros-2-topics",children:"Step 2: Inspect ROS 2 Topics"}),"\n",(0,o.jsxs)(n.p,{children:["While the simulation is running, open a new terminal and source your ROS 2 environment. Then, use ",(0,o.jsx)(n.code,{children:"ros2 topic list"})," to see all active topics:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash # or your ROS 2 setup file\nros2 topic list\n"})}),"\n",(0,o.jsx)(n.p,{children:"You should see topics like:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"/cmd_vel"})," (for sending velocity commands to the robot)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"/odom"})," (odometry data)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"/scan"})," (laser scan data)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"/camera/image_raw"})," (raw camera image)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"/camera/depth/image_raw"})," (raw depth image, if configured)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"/tf"})," and ",(0,o.jsx)(n.code,{children:"/tf_static"})," (robot transformations)"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"step-3-analyze-sensor-data",children:"Step 3: Analyze Sensor Data"}),"\n",(0,o.jsxs)(n.p,{children:["Use ",(0,o.jsx)(n.code,{children:"ros2 topic echo"})," to inspect the data published on various sensor topics."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Odometry Data:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /odom\n"})}),"\n",(0,o.jsxs)(n.p,{children:["You will see ",(0,o.jsx)(n.code,{children:"nav_msgs/msg/Odometry"})," messages, containing the robot's pose (position and orientation) and twist (linear and angular velocities) in the ",(0,o.jsx)(n.code,{children:"odom"})," frame."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Laser Scan Data:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /scan\n"})}),"\n",(0,o.jsxs)(n.p,{children:["You will see ",(0,o.jsx)(n.code,{children:"sensor_msgs/msg/LaserScan"})," messages. Pay attention to the ",(0,o.jsx)(n.code,{children:"ranges"})," array, which contains the distance measurements from the laser scanner."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Camera Image Data (Raw):"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /camera/image_raw\n"})}),"\n",(0,o.jsxs)(n.p,{children:["This will print ",(0,o.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," messages. The output will be very verbose as it's raw pixel data. To visualize images, you typically use a tool like ",(0,o.jsx)(n.code,{children:"rqt_image_view"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"rqt_image_view /camera/image_raw\n"})}),"\n",(0,o.jsx)(n.p,{children:"This will open a GUI displaying the camera feed from the simulated robot."}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"step-4-visualizing-the-robot-and-sensor-data-in-rviz2",children:"Step 4: Visualizing the Robot and Sensor Data in RViz2"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"RViz2"})," is a 3D visualization tool for ROS 2. It's invaluable for understanding what your robot is doing and how its sensors perceive the world."]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Open a new terminal and launch RViz2:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"rviz2\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Add Displays:"}),' In the RViz2 interface, click on "Add" in the "Displays" panel (bottom left).']}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"RobotModel:"})," Add a ",(0,o.jsx)(n.code,{children:"RobotModel"})," display. Set its ",(0,o.jsx)(n.code,{children:"Description Topic"})," to ",(0,o.jsx)(n.code,{children:"/robot_description"})," (or similar) and its ",(0,o.jsx)(n.code,{children:"TF Prefix"})," if your robot uses one. This will show your robot's 3D model."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"TF:"})," Add a ",(0,o.jsx)(n.code,{children:"TF"})," display to visualize the coordinate frames."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LaserScan:"})," Add a ",(0,o.jsx)(n.code,{children:"LaserScan"})," display and set its ",(0,o.jsx)(n.code,{children:"Topic"})," to ",(0,o.jsx)(n.code,{children:"/scan"}),". You should see the laser scanner's readings as points in the 3D view."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Image:"})," Add an ",(0,o.jsx)(n.code,{children:"Image"})," display and set its ",(0,o.jsx)(n.code,{children:"Topic"})," to ",(0,o.jsx)(n.code,{children:"/camera/image_raw"}),". You'll see the camera feed."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Odometry:"})," Add an ",(0,o.jsx)(n.code,{children:"Odometry"})," display and set its ",(0,o.jsx)(n.code,{children:"Topic"})," to ",(0,o.jsx)(n.code,{children:"/odom"}),". You'll see an arrow representing the robot's pose and a trail."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Drive the Robot:"})," While RViz2 and Gazebo are running, use your ",(0,o.jsx)(n.code,{children:"velocity_commander"})," node from Lab 2.1 or directly publish ",(0,o.jsx)(n.code,{children:"Twist"})," messages to ",(0,o.jsx)(n.code,{children:"/cmd_vel"})," to drive the robot:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'ros2 topic pub --once /cmd_vel geometry_msgs/msg/Twist "{linear: {x: 0.1, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}"\n'})}),"\n",(0,o.jsx)(n.p,{children:"Observe how the robot moves in Gazebo, and how its representation and sensor data update in RViz2."}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"step-5-recording-data-optional",children:"Step 5: Recording Data (Optional)"}),"\n",(0,o.jsxs)(n.p,{children:["You can record all ROS 2 topics or selected topics using ",(0,o.jsx)(n.code,{children:"ros2 bag"}),". This is useful for replaying simulations and debugging algorithms offline."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Record all topics\nros2 bag record -a\n\n# Record specific topics\nros2 bag record /odom /scan /camera/image_raw\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Press ",(0,o.jsx)(n.code,{children:"Ctrl+C"})," to stop recording. The data will be saved in a directory named ",(0,o.jsx)(n.code,{children:"ros2_bag_DATE_TIME"}),"."]}),"\n",(0,o.jsx)(n.p,{children:"To replay the bag file:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 bag play ros2_bag_DATE_TIME\n"})}),"\n",(0,o.jsx)(n.h2,{id:"experimentation-and-analysis",children:"Experimentation and Analysis"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor Noise:"})," In Gazebo, you can configure noise properties for simulated sensors. Explore how different noise levels affect the quality of your sensor data."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environmental Impact:"})," Place different objects in the Gazebo world and observe how they affect laser scan readings (e.g., reflections, occlusions)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robot Parameters:"})," If you were to modify the robot's physical parameters (e.g., mass, friction), how would it affect its motion and odometry?"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Visualizing TF:"})," Pay close attention to the ",(0,o.jsx)(n.code,{children:"TF"})," frames in RViz2. Understand the relationships between ",(0,o.jsx)(n.code,{children:"/base_link"}),", ",(0,o.jsx)(n.code,{children:"/odom"}),", ",(0,o.jsx)(n.code,{children:"/camera_link"}),", ",(0,o.jsx)(n.code,{children:"/laser_link"}),", etc."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This lab provides hands-on experience in generating and analyzing simulated sensor data, a fundamental skill for anyone working with robot perception and autonomous navigation."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453(e,n,s){s.d(n,{R:()=>r,x:()=>t});var i=s(6540);const o={},a=i.createContext(o);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);