"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8959],{7945:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module4/index","title":"Module 4: Vision-Language-Action (VLA)","description":"In this final module, we will explore the exciting field of Vision-Language-Action (VLA) models and how they are enabling more natural and intuitive ways to interact with robots.","source":"@site/docs/module4/index.md","sourceDirName":"module4","slug":"/module4/","permalink":"/humanoid-robotics/ur/docs/module4/","draft":false,"unlisted":false,"editUrl":"https://github.com/mishababar12/humanoid-robotics/tree/main/my-website/docs/module4/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 Outline: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/humanoid-robotics/ur/docs/module3/outline"},"next":{"title":"Module 4 Exercises","permalink":"/humanoid-robotics/ur/docs/module4/exercises"}}');var i=n(4848),a=n(8453);const s={sidebar_position:1},r="Module 4: Vision-Language-Action (VLA)",l={},d=[{value:"Introduction to Vision-Language-Action (VLA)",id:"introduction-to-vision-language-action-vla",level:2},{value:"What are VLAs?",id:"what-are-vlas",level:3},{value:"The role of LLMs in robotics",id:"the-role-of-llms-in-robotics",level:3},{value:"Examples of VLA systems",id:"examples-of-vla-systems",level:3},{value:"Integrating LLMs with Robotics",id:"integrating-llms-with-robotics",level:2},{value:"Using LLM APIs",id:"using-llm-apis",level:3},{value:"Prompt engineering for robotics tasks",id:"prompt-engineering-for-robotics-tasks",level:3},{value:"Connecting LLM outputs to robot actions",id:"connecting-llm-outputs-to-robot-actions",level:3},{value:"Human-Robot Interaction (HRI)",id:"human-robot-interaction-hri",level:2},{value:"Principles of HRI",id:"principles-of-hri",level:3},{value:"Natural language as an interface for robots",id:"natural-language-as-an-interface-for-robots",level:3},{value:"Building a Simple VLA System",id:"building-a-simple-vla-system",level:2},{value:"System architecture",id:"system-architecture",level:3},{value:"Integrating a pre-trained VLA model",id:"integrating-a-pre-trained-vla-model",level:3},{value:"Controlling a simulated robot with text commands",id:"controlling-a-simulated-robot-with-text-commands",level:3}];function c(e){const t={h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,i.jsx)(t.p,{children:"In this final module, we will explore the exciting field of Vision-Language-Action (VLA) models and how they are enabling more natural and intuitive ways to interact with robots."}),"\n",(0,i.jsx)(t.h2,{id:"introduction-to-vision-language-action-vla",children:"Introduction to Vision-Language-Action (VLA)"}),"\n",(0,i.jsx)(t.h3,{id:"what-are-vlas",children:"What are VLAs?"}),"\n",(0,i.jsx)(t.p,{children:"Vision-Language-Action (VLA) models are AI models that can understand and respond to instructions in natural language, and can also perceive the world through vision and take actions in it. They are a key enabling technology for building robots that can be instructed by humans in a natural way."}),"\n",(0,i.jsx)(t.h3,{id:"the-role-of-llms-in-robotics",children:"The role of LLMs in robotics"}),"\n",(0,i.jsx)(t.p,{children:"Large Language Models (LLMs) are a type of AI model that can understand and generate human-like text. They are a key component of VLA models, as they provide the language understanding capabilities."}),"\n",(0,i.jsx)(t.h3,{id:"examples-of-vla-systems",children:"Examples of VLA systems"}),"\n",(0,i.jsx)(t.p,{children:'There are many examples of VLA systems being developed today, from research projects to commercial products. Some examples include robots that can be instructed to "pick up the red ball" or "go to the kitchen".'}),"\n",(0,i.jsx)(t.h2,{id:"integrating-llms-with-robotics",children:"Integrating LLMs with Robotics"}),"\n",(0,i.jsx)(t.h3,{id:"using-llm-apis",children:"Using LLM APIs"}),"\n",(0,i.jsx)(t.p,{children:"Many LLMs are available as APIs, which makes it easy to integrate them into your robotics applications. You can send a text prompt to the API and get back a response from the LLM."}),"\n",(0,i.jsx)(t.h3,{id:"prompt-engineering-for-robotics-tasks",children:"Prompt engineering for robotics tasks"}),"\n",(0,i.jsx)(t.p,{children:"Prompt engineering is the art of crafting prompts for LLMs that will elicit the desired response. For robotics tasks, you will need to design prompts that clearly specify the task you want the robot to perform."}),"\n",(0,i.jsx)(t.h3,{id:"connecting-llm-outputs-to-robot-actions",children:"Connecting LLM outputs to robot actions"}),"\n",(0,i.jsx)(t.p,{children:"Once you have a response from the LLM, you will need to translate it into a sequence of robot actions. This can be a challenging task, as the LLM's response may not be specific enough to be directly translated into robot commands."}),"\n",(0,i.jsx)(t.h2,{id:"human-robot-interaction-hri",children:"Human-Robot Interaction (HRI)"}),"\n",(0,i.jsx)(t.h3,{id:"principles-of-hri",children:"Principles of HRI"}),"\n",(0,i.jsx)(t.p,{children:"Human-Robot Interaction (HRI) is the study of how humans and robots can interact with each other in a safe, effective, and enjoyable way."}),"\n",(0,i.jsx)(t.h3,{id:"natural-language-as-an-interface-for-robots",children:"Natural language as an interface for robots"}),"\n",(0,i.jsx)(t.p,{children:"Natural language is a powerful and intuitive way for humans to interact with robots. It allows humans to instruct robots in a way that is similar to how they would instruct other humans."}),"\n",(0,i.jsx)(t.h2,{id:"building-a-simple-vla-system",children:"Building a Simple VLA System"}),"\n",(0,i.jsx)(t.h3,{id:"system-architecture",children:"System architecture"}),"\n",(0,i.jsx)(t.p,{children:"A simple VLA system might consist of a camera, a microphone, an LLM, and a robot. The camera and microphone are used to perceive the world, the LLM is used to understand the user's instructions, and the robot is used to take actions."}),"\n",(0,i.jsx)(t.h3,{id:"integrating-a-pre-trained-vla-model",children:"Integrating a pre-trained VLA model"}),"\n",(0,i.jsx)(t.p,{children:"There are many pre-trained VLA models available that you can use in your own applications. These models have been trained on large datasets of text, images, and actions, and can be fine-tuned for your specific task."}),"\n",(0,i.jsx)(t.h3,{id:"controlling-a-simulated-robot-with-text-commands",children:"Controlling a simulated robot with text commands"}),"\n",(0,i.jsx)(t.p,{children:"In this module, you will learn how to build a simple VLA system that allows you to control a simulated robot with text commands."})]})}function u(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>r});var o=n(6540);const i={},a=o.createContext(i);function s(e){const t=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:t},e.children)}}}]);