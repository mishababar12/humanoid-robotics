"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1112],{1715:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"vla/labs/lab5.1","title":"Lab 5.1: Translating Natural Language Instructions to Robot Actions with an LLM","description":"This lab demonstrates how to use a Large Language Model (LLM) to interpret natural language instructions and translate them into a sequence of executable ROS 2 actions for a simulated robot. This bridges the gap between high-level human commands and low-level robot control.","source":"@site/docs/vla/labs/lab5.1.md","sourceDirName":"vla/labs","slug":"/vla/labs/lab5.1","permalink":"/humanoid-robotics/ur/docs/vla/labs/lab5.1","draft":false,"unlisted":false,"editUrl":"https://github.com/mishababar12/humanoid-robotics/tree/main/my-website/docs/vla/labs/lab5.1.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"VLA Systems & Conversational AI: Bridging the Human-Robot Communication Gap","permalink":"/humanoid-robotics/ur/docs/vla/"},"next":{"title":"LLM Integration for Robot Cognition: Enabling High-Level Reasoning","permalink":"/humanoid-robotics/ur/docs/vla/llm_integration"}}');var s=t(4848),r=t(8453);const a={},i="Lab 5.1: Translating Natural Language Instructions to Robot Actions with an LLM",l={},c=[{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Instructions",id:"instructions",level:2},{value:"Step 1: Set Up Your API Key",id:"step-1-set-up-your-api-key",level:3},{value:"Step 2: Create a ROS 2 Python Package",id:"step-2-create-a-ros-2-python-package",level:3},{value:"Step 3: Implement the LLM Commander Node",id:"step-3-implement-the-llm-commander-node",level:3},{value:"Step 4: Update <code>setup.py</code>",id:"step-4-update-setuppy",level:3},{value:"Step 5: Build Your Package",id:"step-5-build-your-package",level:3},{value:"Step 6: Run the Lab",id:"step-6-run-the-lab",level:3},{value:"Experimentation and Verification",id:"experimentation-and-verification",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lab-51-translating-natural-language-instructions-to-robot-actions-with-an-llm",children:"Lab 5.1: Translating Natural Language Instructions to Robot Actions with an LLM"})}),"\n",(0,s.jsx)(n.p,{children:"This lab demonstrates how to use a Large Language Model (LLM) to interpret natural language instructions and translate them into a sequence of executable ROS 2 actions for a simulated robot. This bridges the gap between high-level human commands and low-level robot control."}),"\n",(0,s.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the workflow of using an LLM for robotic task planning."}),"\n",(0,s.jsx)(n.li,{children:"Integrate with a conversational AI service (e.g., OpenAI API)."}),"\n",(0,s.jsx)(n.li,{children:"Develop a ROS 2 node that sends natural language commands to an LLM."}),"\n",(0,s.jsxs)(n.li,{children:["Translate LLM responses into ROS 2 ",(0,s.jsx)(n.code,{children:"Twist"})," commands for a simulated robot."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A functional ROS 2 Humble/Iron installation."}),"\n",(0,s.jsxs)(n.li,{children:["A ROS 2 workspace set up (e.g., ",(0,s.jsx)(n.code,{children:"~/ros2_ws"}),")."]}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with ROS 2 publisher-subscriber concepts."}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Access to an LLM API:"})," You will need an API key for a service like OpenAI (GPT-3.5/GPT-4) or Google Gemini."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"A simulated robot:"})," The TurtleBot3 in Gazebo (",(0,s.jsx)(n.code,{children:"ros-humble-turtlebot3-gazebo"}),") is recommended."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"instructions",children:"Instructions"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-set-up-your-api-key",children:"Step 1: Set Up Your API Key"}),"\n",(0,s.jsx)(n.p,{children:"Ensure you have an API key for your chosen LLM service (e.g., OpenAI). It's best practice to set this as an environment variable rather than hardcoding it into your script."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'export OPENAI_API_KEY="YOUR_OPENAI_API_KEY"\n# or for Google Gemini\nexport GEMINI_API_KEY="YOUR_GEMINI_API_KEY"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-create-a-ros-2-python-package",children:"Step 2: Create a ROS 2 Python Package"}),"\n",(0,s.jsxs)(n.p,{children:["Navigate to your ROS 2 workspace ",(0,s.jsx)(n.code,{children:"src"})," directory and create a new package named ",(0,s.jsx)(n.code,{children:"llm_robot_commander"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python llm_robot_commander --dependencies rclpy geometry_msgs\n"})}),"\n",(0,s.jsx)(n.p,{children:"This package will contain our LLM interface and robot commander node."}),"\n",(0,s.jsx)(n.h3,{id:"step-3-implement-the-llm-commander-node",children:"Step 3: Implement the LLM Commander Node"}),"\n",(0,s.jsx)(n.p,{children:"This node will:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Subscribe to a text topic for human commands."}),"\n",(0,s.jsx)(n.li,{children:"Send these commands to the LLM API."}),"\n",(0,s.jsx)(n.li,{children:"Parse the LLM's response to extract robot actions (e.g., move forward, turn left)."}),"\n",(0,s.jsxs)(n.li,{children:["Publish ",(0,s.jsx)(n.code,{children:"Twist"})," messages to the robot's ",(0,s.jsx)(n.code,{children:"/cmd_vel"})," topic."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Create a new Python file ",(0,s.jsx)(n.code,{children:"llm_robot_commander/llm_robot_commander/commander_node.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nimport openai # or google.generativeai for Gemini\nimport os\nimport json\nimport time\n\nclass LLMRobotCommander(Node):\n    def __init__(self):\n        super().__init__(\'llm_robot_commander\')\n\n        # Initialize LLM API\n        # For OpenAI\n        openai.api_key = os.getenv("OPENAI_API_KEY")\n        if not openai.api_key:\n            self.get_logger().error("OPENAI_API_KEY environment variable not set.")\n            rclpy.shutdown()\n            return\n        \n        # # For Google Gemini (uncomment and replace if using Gemini)\n        # import google.generativeai as genai\n        # genai.configure(api_key=os.getenv("GEMINI_API_KEY"))\n        # self.model = genai.GenerativeModel(\'gemini-pro\')\n\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.command_subscription = self.create_subscription(\n            String,\n            \'human_command\',\n            self.command_callback,\n            10)\n        self.get_logger().info(\'LLM Robot Commander Node has been started.\')\n        self.current_action_publisher = self.create_publisher(String, \'robot_action_status\', 10)\n\n\n        self.robot_speed_linear = 0.2 # m/s\n        self.robot_speed_angular = 0.5 # rad/s\n\n        self.command_history = [] # For conversation context\n\n    def get_llm_response(self, prompt):\n        # For OpenAI\n        try:\n            response = openai.chat.completions.create(\n                model="gpt-3.5-turbo", # Or "gpt-4" for more complex tasks\n                messages=[\n                    {"role": "system", "content": "You are a robot assistant. Translate human commands into simple robot actions: forward, backward, turn_left, turn_right, stop. Respond with only a JSON object like {\'action\': \'forward\', \'duration\': 2.0} or {\'action\': \'stop\'}."},\n                    *self.command_history, # Include history\n                    {"role": "user", "content": prompt}\n                ],\n                max_tokens=50,\n                temperature=0.0\n            )\n            # Add user and assistant messages to history\n            self.command_history.append({"role": "user", "content": prompt})\n            self.command_history.append({"role": "assistant", "content": response.choices[0].message.content})\n\n            return response.choices[0].message.content\n        except openai.APIError as e:\n            self.get_logger().error(f"OpenAI API Error: {e}")\n            return None\n        \n        # # For Google Gemini (uncomment to use)\n        # try:\n        #     response = self.model.generate_content([\n        #         "You are a robot assistant. Translate human commands into simple robot actions: forward, backward, turn_left, turn_right, stop. Respond with only a JSON object like {\'action\': \'forward\', \'duration\': 2.0} or {\'action\': \'stop\'}.",\n        #         prompt\n        #     ])\n        #     return response.text\n        # except Exception as e:\n        #     self.get_logger().error(f"Gemini API Error: {e}")\n        #     return None\n\n\n    def execute_robot_action(self, action_data):\n        twist_msg = Twist()\n        action = action_data.get(\'action\')\n        duration = action_data.get(\'duration\', 0.0) # Default duration for non-stop actions\n\n        status_msg = String()\n        status_msg.data = f"Executing: {action}"\n        if duration > 0:\n            status_msg.data += f" for {duration} seconds"\n        self.current_action_publisher.publish(status_msg)\n\n\n        if action == \'forward\':\n            twist_msg.linear.x = self.robot_speed_linear\n        elif action == \'backward\':\n            twist_msg.linear.x = -self.robot_speed_linear\n        elif action == \'turn_left\':\n            twist_msg.angular.z = self.robot_speed_angular\n        elif action == \'turn_right\':\n            twist_msg.angular.z = -self.robot_speed_angular\n        elif action == \'stop\':\n            twist_msg.linear.x = 0.0\n            twist_msg.angular.z = 0.0\n        else:\n            self.get_logger().warn(f"Unknown action: {action}")\n            return\n\n        self.cmd_vel_publisher.publish(twist_msg)\n        self.get_logger().info(f"Published Twist: linear.x={twist_msg.linear.x}, angular.z={twist_msg.angular.z}")\n\n        if duration > 0:\n            time.sleep(duration) # Execute for duration\n            # Stop the robot after the duration\n            stop_msg = Twist()\n            self.cmd_vel_publisher.publish(stop_msg)\n            self.get_logger().info(f"Action \'{action}\' completed. Robot stopped.")\n            status_msg.data = f"Action \'{action}\' completed."\n            self.current_action_publisher.publish(status_msg)\n\n\n    def command_callback(self, msg: String):\n        human_command = msg.data\n        self.get_logger().info(f"Received human command: \'{human_command}\'")\n\n        llm_response_text = self.get_llm_response(human_command)\n        if llm_response_text:\n            self.get_logger().info(f"LLM Raw Response: {llm_response_text}")\n            try:\n                action_data = json.loads(llm_response_text)\n                self.execute_robot_action(action_data)\n            except json.JSONDecodeError as e:\n                self.get_logger().error(f"Failed to parse LLM response as JSON: {e}. Response: {llm_response_text}")\n            except Exception as e:\n                self.get_logger().error(f"Error executing robot action: {e}")\n        else:\n            self.get_logger().warn("LLM did not provide a valid response.")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    llm_robot_commander = LLMRobotCommander()\n    rclpy.spin(llm_robot_commander)\n    llm_robot_commander.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Important:"})," This script uses the ",(0,s.jsx)(n.code,{children:"openai"})," library. Install it with ",(0,s.jsx)(n.code,{children:"pip install openai"}),". If using Google Gemini, install ",(0,s.jsx)(n.code,{children:"google-generativeai"})," with ",(0,s.jsx)(n.code,{children:"pip install google-generativeai"}),". You will also need to manually uncomment the Gemini related code and comment out the OpenAI code."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"time.sleep()"}),":"]})," Using ",(0,s.jsx)(n.code,{children:"time.sleep()"})," in a ROS 2 callback blocks the executor. For real-world applications, you'd integrate action duration into a more sophisticated action server or a state machine. This simple example uses ",(0,s.jsx)(n.code,{children:"time.sleep()"})," for clarity."]}),"\n"]}),"\n",(0,s.jsxs)(n.h3,{id:"step-4-update-setuppy",children:["Step 4: Update ",(0,s.jsx)(n.code,{children:"setup.py"})]}),"\n",(0,s.jsxs)(n.p,{children:["Modify ",(0,s.jsx)(n.code,{children:"llm_robot_commander/setup.py"})," to add the entry point for your new node:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from setuptools import find_packages, setup\n\npackage_name = 'llm_robot_commander'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools', 'openai'], # Add openai here (or google-generativeai for Gemini)\n    zip_safe=True,\n    maintainer='Your Name',\n    maintainer_email='user@example.com',\n    description='TODO: Package description',\n    license='TODO: License declaration',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'commander_node = llm_robot_commander.commander_node:main',\n        ],\n    },\n)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-5-build-your-package",children:"Step 5: Build Your Package"}),"\n",(0,s.jsxs)(n.p,{children:["Build your workspace from the root of your ",(0,s.jsx)(n.code,{children:"ros2_ws"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select llm_robot_commander\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-6-run-the-lab",children:"Step 6: Run the Lab"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Launch the simulated robot (e.g., TurtleBot3):"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Run the LLM Robot Commander Node:"})," In a new terminal:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\nros2 run llm_robot_commander commander_node\n"})}),"\n",(0,s.jsx)(n.p,{children:"Ensure your API key is set as an environment variable before running this."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Send Natural Language Commands:"})," Open another new terminal and publish commands to the ",(0,s.jsx)(n.code,{children:"human_command"})," topic:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Move Forward:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 topic pub --once /human_command std_msgs/msg/String \"data: 'Move forward for 3 seconds'\"\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Turn Left:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 topic pub --once /human_command std_msgs/msg/String \"data: 'Please turn left for 2 seconds'\"\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stop:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 topic pub --once /human_command std_msgs/msg/String \"data: 'Stop the robot'\"\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Complex Command (might require more advanced LLM parsing):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 topic pub --once /human_command std_msgs/msg/String \"data: 'Go forward a bit, then make a right turn'\"\n"})}),"\n","(Note: The current LLM prompt is very simple, so complex chained commands might not work well without refining the prompt or adding more logic to the node.)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"experimentation-and-verification",children:"Experimentation and Verification"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM Prompt Engineering:"})," Experiment with different system prompts for the LLM to guide its output format and behavior. Can you make it respond with more specific actions or handle more complex sequences?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling:"})," What happens if the LLM returns an unexpected format? Enhance the JSON parsing in the ",(0,s.jsx)(n.code,{children:"commander_node.py"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech Interface:"})," Integrate the speech-to-text functionality from Exercise 5.2 into this lab to allow voice commands instead of typing them into ",(0,s.jsx)(n.code,{children:"ros2 topic pub"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advanced Actions:"})," Extend the ",(0,s.jsx)(n.code,{children:"execute_robot_action"})," function to handle more complex robot behaviors beyond simple ",(0,s.jsx)(n.code,{children:"Twist"})," messages (e.g., calling ROS 2 services or actions for picking, placing, etc.)."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This lab provides a powerful demonstration of how conversational AI and LLMs can directly influence robot control, opening up new possibilities for intuitive and flexible human-robot interaction."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>i});var o=t(6540);const s={},r=o.createContext(s);function a(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);