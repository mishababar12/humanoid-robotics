"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9708],{5637(e){e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Physical AI Principles","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/intro/embodied_intelligence","label":"Embodied Intelligence","docId":"intro/embodied_intelligence","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/intro/","label":"Introduction to Physical AI","docId":"intro/index","unlisted":false}],"href":"/humanoid-robotics/ur/docs/category/physical-ai-principles"},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/module1/exercises","label":"Module 1 Exercises","docId":"module1/exercises","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/module1/outline","label":"Module 1 Outline: The Robotic Nervous System (ROS 2)","docId":"module1/outline","unlisted":false}],"href":"/humanoid-robotics/ur/docs/module1/"},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/module2/exercises","label":"Module 2 Exercises","docId":"module2/exercises","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/module2/outline","label":"Module 2 Outline: The Digital Twin (Gazebo & Unity)","docId":"module2/outline","unlisted":false}],"href":"/humanoid-robotics/ur/docs/module2/"},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/module3/exercises","label":"Module 3 Exercises","docId":"module3/exercises","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/module3/outline","label":"Module 3 Outline: The AI-Robot Brain (NVIDIA Isaac\u2122)","docId":"module3/outline","unlisted":false}],"href":"/humanoid-robotics/ur/docs/module3/"},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/module4/exercises","label":"Module 4 Exercises","docId":"module4/exercises","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/module4/outline","label":"Module 4 Outline: Vision-Language-Action (VLA)","docId":"module4/outline","unlisted":false}],"href":"/humanoid-robotics/ur/docs/module4/"},{"type":"category","label":"ROS 2 for Robot Control","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"exercises","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/ros2/exercises/exercise1","label":"Exercise 2.1: Simple ROS 2 Publisher and Subscriber with Custom Message","docId":"ros2/exercises/exercise1","unlisted":false}]},{"type":"link","href":"/humanoid-robotics/ur/docs/ros2/","label":"ROS 2 for Robot Control","docId":"ros2/index","unlisted":false},{"type":"category","label":"labs","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/ros2/labs/lab1","label":"Lab 2.1: Basic Robot Control in Simulation","docId":"ros2/labs/lab1","unlisted":false}]},{"type":"link","href":"/humanoid-robotics/ur/docs/ros2/week1_overview","label":"ROS 2: Week 1 Overview - Getting Started with ROS 2","docId":"ros2/week1_overview","unlisted":false}],"href":"/humanoid-robotics/ur/docs/category/ros-2-for-robot-control"},{"type":"category","label":"Tutorial - Basics","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/tutorial-basics/create-a-page","label":"Create a Page","docId":"tutorial-basics/create-a-page","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/tutorial-basics/create-a-document","label":"Create a Document","docId":"tutorial-basics/create-a-document","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/tutorial-basics/create-a-blog-post","label":"Create a Blog Post","docId":"tutorial-basics/create-a-blog-post","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/tutorial-basics/markdown-features","label":"Markdown Features","docId":"tutorial-basics/markdown-features","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/tutorial-basics/deploy-your-site","label":"Deploy your site","docId":"tutorial-basics/deploy-your-site","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/tutorial-basics/congratulations","label":"Congratulations!","docId":"tutorial-basics/congratulations","unlisted":false}],"href":"/humanoid-robotics/ur/docs/category/tutorial---basics"},{"type":"category","label":"Robot Simulation with Gazebo & Unity","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/gazebo_unity/gazebo_environments","label":"Gazebo Environments: Building Your Simulated Robotic Worlds","docId":"gazebo_unity/gazebo_environments","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/gazebo_unity/","label":"Robot Simulation with Gazebo & Unity","docId":"gazebo_unity/index","unlisted":false},{"type":"category","label":"labs","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/gazebo_unity/labs/lab3.1","label":"Lab 3.1: Simulating a Mobile Robot and Streaming Sensor Data in Gazebo","docId":"gazebo_unity/labs/lab3.1","unlisted":false}]},{"type":"link","href":"/humanoid-robotics/ur/docs/gazebo_unity/unity_visualization","label":"Unity Visualization: Bringing Robots to Life with Advanced Graphics","docId":"gazebo_unity/unity_visualization","unlisted":false}],"href":"/humanoid-robotics/ur/docs/category/robot-simulation-with-gazebo--unity"},{"type":"category","label":"Tutorial - Extras","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/tutorial-extras/manage-docs-versions","label":"Manage Docs Versions","docId":"tutorial-extras/manage-docs-versions","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/tutorial-extras/translate-your-site","label":"Translate your site","docId":"tutorial-extras/translate-your-site","unlisted":false}],"href":"/humanoid-robotics/ur/docs/category/tutorial---extras"},{"type":"category","label":"NVIDIA Isaac AI Platform","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/isaac/","label":"NVIDIA Isaac AI Platform: Accelerating AI in Robotics","docId":"isaac/index","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/isaac/perception_pipeline","label":"Isaac Perception Pipeline: Giving Robots the Sense of Sight","docId":"isaac/perception_pipeline","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/isaac/reinforcement_learning","label":"Reinforcement Learning with Isaac Sim: Training Intelligent Robot Behaviors","docId":"isaac/reinforcement_learning","unlisted":false}],"href":"/humanoid-robotics/ur/docs/category/nvidia-isaac-ai-platform"},{"type":"category","label":"VLA Systems & Conversational AI","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/vla/human_robot_interaction","label":"Human-Robot Interaction (HRI): Designing for Seamless Collaboration","docId":"vla/human_robot_interaction","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/vla/","label":"VLA Systems & Conversational AI: Bridging the Human-Robot Communication Gap","docId":"vla/index","unlisted":false},{"type":"category","label":"labs","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/vla/labs/lab5.1","label":"Lab 5.1: Translating Natural Language Instructions to Robot Actions with an LLM","docId":"vla/labs/lab5.1","unlisted":false}]},{"type":"link","href":"/humanoid-robotics/ur/docs/vla/llm_integration","label":"LLM Integration for Robot Cognition: Enabling High-Level Reasoning","docId":"vla/llm_integration","unlisted":false}],"href":"/humanoid-robotics/ur/docs/category/vla-systems--conversational-ai"},{"type":"category","label":"Weekly Course Breakdown","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week1","label":"Week 1: Introduction to Physical AI and Embodied Systems","docId":"weekly_breakdown/week1","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week10","label":"Week 10: Introduction to Robot Learning","docId":"weekly_breakdown/week10","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week11","label":"Week 11: Introduction to Vision-Language-Action (VLA)","docId":"weekly_breakdown/week11","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week12","label":"Week 12: Integrating LLMs with Robotics","docId":"weekly_breakdown/week12","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week13","label":"Week 13: Human-Robot Interaction and Project Showcase","docId":"weekly_breakdown/week13","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week2","label":"Week 2: Introduction to ROS 2","docId":"weekly_breakdown/week2","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week3","label":"Week 3: ROS 2 Services and rclpy","docId":"weekly_breakdown/week3","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week4","label":"Week 4: Robot Modeling with URDF","docId":"weekly_breakdown/week4","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week5","label":"Week 5: Introduction to Digital Twins and Gazebo","docId":"weekly_breakdown/week5","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week6","label":"Week 6: ROS 2 and Gazebo Integration","docId":"weekly_breakdown/week6","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week7","label":"Week 7: High-Quality Visualization with Unity","docId":"weekly_breakdown/week7","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week8","label":"Week 8: Introduction to NVIDIA Isaac","docId":"weekly_breakdown/week8","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/weekly_breakdown/week9","label":"Week 9: AI and Perception with Isaac ROS","docId":"weekly_breakdown/week9","unlisted":false}],"href":"/humanoid-robotics/ur/docs/category/weekly-course-breakdown"},{"type":"category","label":"Hardware & Software Requirements","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/hardware_software_requirements/hardware","label":"Hardware Requirements","docId":"hardware_software_requirements/hardware","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/hardware_software_requirements/software","label":"Software Requirements","docId":"hardware_software_requirements/software","unlisted":false}],"href":"/humanoid-robotics/ur/docs/category/hardware--software-requirements"},{"type":"category","label":"Capstone Humanoid Project","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-robotics/ur/docs/capstone_project/","label":"Capstone Humanoid Project","docId":"capstone_project/index","unlisted":false}],"href":"/humanoid-robotics/ur/docs/category/capstone-humanoid-project"},{"type":"link","href":"/humanoid-robotics/ur/docs/chatbot-test","label":"RAG Chatbot Test","docId":"chatbot-test","unlisted":false},{"type":"link","href":"/humanoid-robotics/ur/docs/intro","label":"Introduction to Physical AI","docId":"intro","unlisted":false}]},"docs":{"capstone_project/index":{"id":"capstone_project/index","title":"Capstone Humanoid Project","description":"This capstone project integrates all the knowledge and skills acquired throughout the course to develop a comprehensive solution for a simulated humanoid robot.","sidebar":"tutorialSidebar"},"chatbot-test":{"id":"chatbot-test","title":"RAG Chatbot Test","description":"This page is designed to test the RAG chatbot functionality. The chatbot should appear as a floating widget on the bottom right of the screen.","sidebar":"tutorialSidebar"},"gazebo_unity/gazebo_environments":{"id":"gazebo_unity/gazebo_environments","title":"Gazebo Environments: Building Your Simulated Robotic Worlds","description":"Gazebo is a powerful 3D robot simulator that provides the ability to accurately and efficiently simulate robots and their environments. It is an essential tool for robotics research and development, allowing for the testing of complex algorithms and control systems in a virtual setting before deployment to physical hardware. This section will guide you through the process of setting up and customizing Gazebo environments.","sidebar":"tutorialSidebar"},"gazebo_unity/index":{"id":"gazebo_unity/index","title":"Robot Simulation with Gazebo & Unity","description":"This module delves into the critical role of robot simulation in the development lifecycle of physical AI systems. Simulators provide a safe, cost-effective, and reproducible environment for designing, testing, and debugging robot hardware, software, and control algorithms before deploying them to real-world robots. We will explore two powerful and widely-used simulation platforms: Gazebo and Unity.","sidebar":"tutorialSidebar"},"gazebo_unity/labs/lab3.1":{"id":"gazebo_unity/labs/lab3.1","title":"Lab 3.1: Simulating a Mobile Robot and Streaming Sensor Data in Gazebo","description":"This lab focuses on setting up a Gazebo simulation for a mobile robot and configuring it to stream various sensor data to ROS 2 topics. This is a crucial step for developing and testing robot perception and navigation algorithms.","sidebar":"tutorialSidebar"},"gazebo_unity/unity_visualization":{"id":"gazebo_unity/unity_visualization","title":"Unity Visualization: Bringing Robots to Life with Advanced Graphics","description":"While Gazebo provides robust physics-based simulation, Unity offers unparalleled capabilities for creating highly realistic and interactive 3D visualizations. This makes Unity an excellent platform for visualizing complex robot models, sensor data, and creating rich human-robot interaction interfaces, often complementing Gazebo or serving as a standalone simulation environment.","sidebar":"tutorialSidebar"},"hardware_software_requirements/hardware":{"id":"hardware_software_requirements/hardware","title":"Hardware Requirements","description":"To effectively participate in this course and run the simulations and exercises, the following hardware specifications are recommended:","sidebar":"tutorialSidebar"},"hardware_software_requirements/software":{"id":"hardware_software_requirements/software","title":"Software Requirements","description":"The following software will be used throughout the course. Please ensure you have these installed and configured correctly.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction to Physical AI","description":"Welcome to the Physical AI & Humanoid Robotics Textbook, your AI-native guide to understanding, designing, and experimenting with physical AI systems and humanoid robots. This textbook has been created as part of a comprehensive course using Docusaurus, integrated with interactive exercises, code examples, diagrams, and a RAG-powered AI assistant.","sidebar":"tutorialSidebar"},"intro/embodied_intelligence":{"id":"intro/embodied_intelligence","title":"Embodied Intelligence","description":"Embodied intelligence is a paradigm in artificial intelligence and cognitive science that posits an intelligent agent\'s capabilities and cognitive processes are not solely a product of its brain or central processing unit, but are deeply intertwined with its physical body, its sensory experiences, and its dynamic interactions with the environment. It challenges the traditional view of intelligence as purely abstract and disembodied computation.","sidebar":"tutorialSidebar"},"intro/index":{"id":"intro/index","title":"Introduction to Physical AI","description":"This module introduces the foundational principles of Physical AI and embodied intelligence, laying the groundwork for understanding how artificial intelligence can manifest and operate within the physical world through robotic systems. Unlike traditional AI, which often operates in abstract, disembodied environments (e.g., chess engines, natural language processing models), Physical AI focuses on agents that perceive, reason, and act within real or simulated physical spaces.","sidebar":"tutorialSidebar"},"isaac/index":{"id":"isaac/index","title":"NVIDIA Isaac AI Platform: Accelerating AI in Robotics","description":"This module delves into the NVIDIA Isaac AI Platform, a comprehensive suite of hardware, software, and simulation tools designed to accelerate the development and deployment of AI-powered robots. Isaac is at the forefront of enabling advanced capabilities in areas such as perception, navigation, manipulation, and human-robot interaction, pushing the boundaries of what autonomous systems can achieve.","sidebar":"tutorialSidebar"},"isaac/perception_pipeline":{"id":"isaac/perception_pipeline","title":"Isaac Perception Pipeline: Giving Robots the Sense of Sight","description":"The NVIDIA Isaac SDK provides a powerful framework for building robust and efficient perception pipelines, enabling robots to interpret and understand their environment. Perception is a cornerstone of any autonomous system, allowing robots to locate themselves, map their surroundings, identify objects, and avoid obstacles. This section will delve into the key components and methodologies for constructing Isaac-based perception pipelines.","sidebar":"tutorialSidebar"},"isaac/reinforcement_learning":{"id":"isaac/reinforcement_learning","title":"Reinforcement Learning with Isaac Sim: Training Intelligent Robot Behaviors","description":"NVIDIA Isaac Sim, built on the Omniverse platform, is a powerful, physically accurate simulation environment that has become a cornerstone for training intelligent robot behaviors using Reinforcement Learning (RL). RL is a machine learning paradigm where an agent learns to make decisions by interacting with an environment, receiving rewards for desirable actions and penalties for undesirable ones. Isaac Sim accelerates this process by providing highly parallelizable and realistic simulation capabilities.","sidebar":"tutorialSidebar"},"module1/exercises":{"id":"module1/exercises","title":"Module 1 Exercises","description":"These exercises are designed to accompany Module 1 of the \\"Physical AI & Humanoid Robotics\\" textbook.","sidebar":"tutorialSidebar"},"module1/index":{"id":"module1/index","title":"Module 1: The Robotic Nervous System (ROS 2)","description":"Welcome to Module 1 of the \\"Physical AI & Humanoid Robotics\\" textbook. In this module, you will learn about the Robot Operating System 2 (ROS 2), the foundational software framework for modern robotics. By the end of this module, you will be able to build and run your own simple robotic applications.","sidebar":"tutorialSidebar"},"module1/outline":{"id":"module1/outline","title":"Module 1 Outline: The Robotic Nervous System (ROS 2)","description":"This document provides a detailed outline for Module 1 of the \\"Physical AI & Humanoid Robotics\\" textbook.","sidebar":"tutorialSidebar"},"module2/exercises":{"id":"module2/exercises","title":"Module 2 Exercises","description":"These exercises are designed to accompany Module 2 of the \\"Physical AI & Humanoid Robotics\\" textbook.","sidebar":"tutorialSidebar"},"module2/index":{"id":"module2/index","title":"Module 2: The Digital Twin (Gazebo & Unity)","description":"In this module, you will learn about the concept of a \\"Digital Twin\\" and how to use simulation tools like Gazebo and Unity to create realistic virtual environments for testing and developing your robotic applications.","sidebar":"tutorialSidebar"},"module2/outline":{"id":"module2/outline","title":"Module 2 Outline: The Digital Twin (Gazebo & Unity)","description":"This document provides a detailed outline for Module 2 of the \\"Physical AI & Humanoid Robotics\\" textbook.","sidebar":"tutorialSidebar"},"module3/exercises":{"id":"module3/exercises","title":"Module 3 Exercises","description":"These exercises are designed to accompany Module 3 of the \\"Physical AI & Humanoid Robotics\\" textbook.","sidebar":"tutorialSidebar"},"module3/index":{"id":"module3/index","title":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","description":"In this module, you will dive into the world of AI-powered robotics with NVIDIA Isaac, a powerful platform for developing and deploying intelligent robots.","sidebar":"tutorialSidebar"},"module3/outline":{"id":"module3/outline","title":"Module 3 Outline: The AI-Robot Brain (NVIDIA Isaac\u2122)","description":"This document provides a detailed outline for Module 3 of the \\"Physical AI & Humanoid Robotics\\" textbook.","sidebar":"tutorialSidebar"},"module4/exercises":{"id":"module4/exercises","title":"Module 4 Exercises","description":"These exercises are designed to accompany Module 4 of the \\"Physical AI & Humanoid Robotics\\" textbook.","sidebar":"tutorialSidebar"},"module4/index":{"id":"module4/index","title":"Module 4: Vision-Language-Action (VLA)","description":"In this final module, we will explore the exciting field of Vision-Language-Action (VLA) models and how they are enabling more natural and intuitive ways to interact with robots.","sidebar":"tutorialSidebar"},"module4/outline":{"id":"module4/outline","title":"Module 4 Outline: Vision-Language-Action (VLA)","description":"This document provides a detailed outline for Module 4 of the \\"Physical AI & Humanoid Robotics\\" textbook.","sidebar":"tutorialSidebar"},"ros2/exercises/exercise1":{"id":"ros2/exercises/exercise1","title":"Exercise 2.1: Simple ROS 2 Publisher and Subscriber with Custom Message","description":"This exercise guides you through creating a basic ROS 2 publisher and subscriber using Python, incorporating a custom message type. This will solidify your understanding of ROS 2 package creation, message definition, and the fundamental publisher-subscriber communication pattern.","sidebar":"tutorialSidebar"},"ros2/index":{"id":"ros2/index","title":"ROS 2 for Robot Control","description":"This module focuses on mastering the Robot Operating System 2 (ROS 2) for developing robust and scalable robotic applications. ROS 2 is a flexible framework for writing robot software, offering a collection of tools, libraries, and conventions that simplify the complex task of building robots. It enables developers to create modular and distributed systems, crucial for the advanced physical AI systems we aim to build.","sidebar":"tutorialSidebar"},"ros2/labs/lab1":{"id":"ros2/labs/lab1","title":"Lab 2.1: Basic Robot Control in Simulation","description":"This lab will guide you through controlling a simulated robot using ROS 2. You will learn to send commands to make the robot move and receive feedback on its position and orientation, providing a fundamental understanding of mobile robot control.","sidebar":"tutorialSidebar"},"ros2/week1_overview":{"id":"ros2/week1_overview","title":"ROS 2: Week 1 Overview - Getting Started with ROS 2","description":"This week serves as your foundational introduction to the Robot Operating System 2 (ROS 2). We will cover everything from setting up your development environment to understanding the core communication mechanisms that make ROS 2 so powerful for robotics development. By the end of this week, you will be able to set up a ROS 2 workspace, create basic ROS 2 packages, and implement simple publisher and subscriber nodes.","sidebar":"tutorialSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template.","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:","sidebar":"tutorialSidebar"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack).","sidebar":"tutorialSidebar"},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features.","sidebar":"tutorialSidebar"},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs.","sidebar":"tutorialSidebar"},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French.","sidebar":"tutorialSidebar"},"vla/human_robot_interaction":{"id":"vla/human_robot_interaction","title":"Human-Robot Interaction (HRI): Designing for Seamless Collaboration","description":"Human-Robot Interaction (HRI) is a field of study dedicated to understanding, designing, and evaluating robotic systems for use by or with humans. As robots become more prevalent in our daily lives, from industrial settings to homes and public spaces, the quality of their interaction with humans becomes paramount. Integrating Vision-Language-Action (VLA) systems and conversational AI significantly elevates the naturalness and effectiveness of HRI, fostering more intuitive and productive collaboration.","sidebar":"tutorialSidebar"},"vla/index":{"id":"vla/index","title":"VLA Systems & Conversational AI: Bridging the Human-Robot Communication Gap","description":"This module focuses on the exciting frontier of integrating Vision-Language-Action (VLA) systems and conversational AI into robotics. The goal is to enable more intuitive, flexible, and natural human-robot interactions, moving beyond predefined commands to a state where robots can understand and respond to human intent expressed through natural language and visual cues.","sidebar":"tutorialSidebar"},"vla/labs/lab5.1":{"id":"vla/labs/lab5.1","title":"Lab 5.1: Translating Natural Language Instructions to Robot Actions with an LLM","description":"This lab demonstrates how to use a Large Language Model (LLM) to interpret natural language instructions and translate them into a sequence of executable ROS 2 actions for a simulated robot. This bridges the gap between high-level human commands and low-level robot control.","sidebar":"tutorialSidebar"},"vla/llm_integration":{"id":"vla/llm_integration","title":"LLM Integration for Robot Cognition: Enabling High-Level Reasoning","description":"Large Language Models (LLMs) are revolutionizing how robots can process and respond to complex information, moving beyond predefined scripts to truly understand and execute natural language instructions. Integrating LLMs into robot cognitive architectures provides advanced capabilities for high-level planning, decision-making, and bridging the gap between human intent and robot action.","sidebar":"tutorialSidebar"},"weekly_breakdown/week1":{"id":"weekly_breakdown/week1","title":"Week 1: Introduction to Physical AI and Embodied Systems","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week10":{"id":"weekly_breakdown/week10","title":"Week 10: Introduction to Robot Learning","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week11":{"id":"weekly_breakdown/week11","title":"Week 11: Introduction to Vision-Language-Action (VLA)","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week12":{"id":"weekly_breakdown/week12","title":"Week 12: Integrating LLMs with Robotics","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week13":{"id":"weekly_breakdown/week13","title":"Week 13: Human-Robot Interaction and Project Showcase","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week2":{"id":"weekly_breakdown/week2","title":"Week 2: Introduction to ROS 2","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week3":{"id":"weekly_breakdown/week3","title":"Week 3: ROS 2 Services and rclpy","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week4":{"id":"weekly_breakdown/week4","title":"Week 4: Robot Modeling with URDF","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week5":{"id":"weekly_breakdown/week5","title":"Week 5: Introduction to Digital Twins and Gazebo","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week6":{"id":"weekly_breakdown/week6","title":"Week 6: ROS 2 and Gazebo Integration","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week7":{"id":"weekly_breakdown/week7","title":"Week 7: High-Quality Visualization with Unity","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week8":{"id":"weekly_breakdown/week8","title":"Week 8: Introduction to NVIDIA Isaac","description":"Module Covered","sidebar":"tutorialSidebar"},"weekly_breakdown/week9":{"id":"weekly_breakdown/week9","title":"Week 9: AI and Perception with Isaac ROS","description":"Module Covered","sidebar":"tutorialSidebar"}}}}')}}]);